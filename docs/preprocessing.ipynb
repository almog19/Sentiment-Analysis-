{
"nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "00y2wgcp5Sqa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "#preprocessing data(cleaning):\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "#vectorization(TF-IDF):\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#train:\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#evaluation:\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n"
      ],
      "metadata": {
        "id": "7brQx4x1Pzo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###clean the data"
      ],
      "metadata": {
        "id": "XAIO_0hWFXGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- lowercasing\n",
        "\n",
        "- remove links\n",
        "\n",
        "- remove special characters\n",
        "\n",
        "         .,@!<\n",
        "\n",
        "- remove stopword, *is not implemented due to making the model confused.*\n",
        "\n",
        "        is, so, the, a\n",
        "\n",
        "- tokenization: split into individual words\n",
        "\n",
        "- lemmatization: reduce words to their root(real dictionary form)\n",
        "\n",
        "        running -> run,"
      ],
      "metadata": {
        "id": "uF9A-XGIFotz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    tagged = pos_tag(words)  # [('going','VBG'), ('better','JJR'),...]\n",
        "    lemmas = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
        "        for word, tag in tagged\n",
        "    ]\n",
        "    return lemmas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBVyrKTx9O2c",
        "outputId": "2c02130b-380f-4c58-bdb6-6c5cae4be4f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocessing(text):\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "    # 3. Remove usernames/mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # 4. Replace long repeated letters (\"soooo\" → \"soo\")\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "    # 5. Remove special characters & numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "    # 6. Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # 7. Tokenize\n",
        "    words = text.split()\n",
        "\n",
        "    # 8. Remove stopwords (optional)\n",
        "    # 9. Lemmatize (convert to root form)\n",
        "\n",
        "    words = lemmatize_words(words)\n",
        "\n",
        "    # 10. Rejoin to text\n",
        "    return ' '.join(words)\n"
      ],
      "metadata": {
        "id": "tL-H4D-kE7z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = []\n",
        "for text in tqdm(X):\n",
        "  clean_text.append(preprocessing(text))\n",
        "print(\"\\n\", len(clean_text))"
      ],
      "metadata": {
        "id": "pkeutGYoNnvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f0c5de5-adc0-4bcf-dfa6-d8818e1daad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30499/30499 [00:21<00:00, 1422.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 30499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = np.zeros(len(classes))\n",
        "for i in y:\n",
        "  label[int(i*(len(classes)-1))]+=1\n",
        "print(label)\n",
        "plt.bar(classes,label)\n",
        "plt.xlabel(\"semantic evaluation\")\n",
        "plt.ylabel(\"examples\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rw0o0EUPne_",
        "outputId": "7bf7dd6a-4e59-4f96-aea6-2e80f0a89f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10134. 10166. 10199.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": ""
          },
          "metadata": {}
        }
      ]
    }
  ]
}
